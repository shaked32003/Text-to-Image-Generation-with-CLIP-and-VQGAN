{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PYJKNhOl0KZ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/CompVis/taming-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHwdD4U5qICi"
      },
      "outputs": [],
      "source": [
        "!pip install --no-deps ftfy regex tqdm\n",
        "!pip install omegaconf==2.0.0 pytorch-lightning==1.0.8\n",
        "!pip uninstall torchtext --yes\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGqif3JHqqwf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch, os, imageio, pdb, math\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import yaml\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from CLIP import clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE0NV2uYq-sa"
      },
      "outputs": [],
      "source": [
        "def show_from_tensor(tensor):\n",
        "  img = tensor.clone()\n",
        "  img = img.mul(255).byte()\n",
        "  img = img.cpu().numpy().transpose(1,2,0)\n",
        "\n",
        "  plt.figure(figsize=(10,7))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "def norm_data(data):\n",
        "  return (data.clip(-1,1)+1)/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtmC87OABjAs"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.5\n",
        "batch_size = 1\n",
        "wd = 0.15\n",
        "noise_factor = 0.22\n",
        "\n",
        "w1=1\n",
        "w2=0.1\n",
        "total_iter=8000\n",
        "show_step=1000\n",
        "\n",
        "im_shape = [450, 450, 3]\n",
        "size1, size2, channels = im_shape\n",
        "\n",
        "device=torch.device(\"cuda:0\")\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW7MAyuWsuta"
      },
      "outputs": [],
      "source": [
        "clipmodel,_ = clip.load('ViT-B/32', jit=False)\n",
        "clipmodel.eval()\n",
        "print(clip.available_models())\n",
        "\n",
        "print(\"Clip model visual input resolution: \", clipmodel.visual.input_resolution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8DDlsvUwVnd"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"taming-transformers/\")\n",
        "\n",
        "!mkdir -p models/vqgan_imagenet_f16_16384/checkpoints\n",
        "!mkdir -p models/vqgan_imagenet_f16_16384/configs\n",
        "\n",
        "if len(os.listdir('models/vqgan_imagenet_f16_16384/checkpoints/')) == 0:\n",
        "   !wget 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1' -O 'models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt'\n",
        "   !wget 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1' -O 'models/vqgan_imagenet_f16_16384/configs/model.yaml'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axoE9XVlxecv"
      },
      "outputs": [],
      "source": [
        "from taming.models.vqgan import VQModel\n",
        "\n",
        "def load_config(config_path, display=False):\n",
        "   config_data = OmegaConf.load(config_path)\n",
        "   if display:\n",
        "     print(yaml.dump(OmegaConf.to_container(config_data)))\n",
        "   return config_data\n",
        "\n",
        "def load_vqgan(config, chk_path=None):\n",
        "  model = VQModel(**config.model.params)\n",
        "  if chk_path is not None:\n",
        "    state_dict = torch.load(chk_path, map_location=\"cpu\")[\"state_dict\"]\n",
        "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "  return model.eval()\n",
        "\n",
        "def generator(x):\n",
        "  x = taming_model.post_quant_conv(x)\n",
        "  x = taming_model.decoder(x)\n",
        "  return x\n",
        "\n",
        "taming_config = load_config(\"./models/vqgan_imagenet_f16_16384/configs/model.yaml\", display=True)\n",
        "taming_model = load_vqgan(taming_config, chk_path=\"./models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt\").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMMgSj-eGYLp"
      },
      "outputs": [],
      "source": [
        "class Parameters(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Parameters,self).__init__()\n",
        "    self.data = .5*torch.randn(batch_size, 256, size1//16, size2//16).cuda()\n",
        "    self.data = torch.nn.Parameter(torch.sin(self.data))\n",
        "\n",
        "  def forward(self):\n",
        "    return self.data\n",
        "\n",
        "def init_params():\n",
        "  params = Parameters().cuda()\n",
        "  optimizer = torch.optim.AdamW([{'params':[params.data], 'lr': learning_rate}], weight_decay=wd)\n",
        "  return params, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksaq1aIqW3IC"
      },
      "outputs": [],
      "source": [
        "\n",
        "normalize = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "def encodeText(text):\n",
        "  t = clip.tokenize(text).cuda()\n",
        "  t = clipmodel.encode_text(t).detach().clone()\n",
        "  return t\n",
        "\n",
        "def createEncodings(include, exclude, extras):\n",
        "  include_enc=[]\n",
        "  for text in include:\n",
        "    include_enc.append(encodeText(text))\n",
        "  exclude_enc=encodeText(exclude) if exclude != '' else torch.tensor([0]).cuda()\n",
        "  extras_enc=encodeText(extras) if extras !='' else torch.tensor([0]).cuda()\n",
        "\n",
        "  return include_enc, exclude_enc, extras_enc\n",
        "\n",
        "augTransform = torch.nn.Sequential(torchvision.transforms.RandomHorizontalFlip()\n",
        "        ,torchvision.transforms.RandomAffine(30, (.2, .2), fill=0)).cuda()\n",
        "\n",
        "Params, optimizer = init_params()\n",
        "\n",
        "with torch.no_grad():\n",
        "  print(Params().shape)\n",
        "  img= norm_data(generator(Params()).cpu())\n",
        "  print(\"img dimensions: \",img.shape)\n",
        "  show_from_tensor(img[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3CPyP_DOI7a"
      },
      "outputs": [],
      "source": [
        "def create_crops(img, num_crops=32):\n",
        "  p=size1//2\n",
        "  img = torch.nn.functional.pad(img, (p,p,p,p), mode='constant', value=0)\n",
        "\n",
        "  img = augTransform(img)\n",
        "\n",
        "  crop_set = []\n",
        "  for ch in range(num_crops):\n",
        "    gap1= int(torch.normal(1.2, .3, ()).clip(.43, 1.9) * size1)\n",
        "    offsetx = torch.randint(0, int(size1*2-gap1),())\n",
        "    offsety = torch.randint(0, int(size1*2-gap1),())\n",
        "\n",
        "    crop=img[:,:,offsetx:offsetx+gap1, offsety:offsety+gap1]\n",
        "\n",
        "    crop = torch.nn.functional.interpolate(crop,(224,224), mode='bilinear', align_corners=True)\n",
        "    crop_set.append(crop)\n",
        "\n",
        "  img_crops=torch.cat(crop_set,0)\n",
        "\n",
        "  randnormal = torch.randn_like(img_crops, requires_grad=False)\n",
        "  num_rands=0\n",
        "  randstotal=torch.rand((img_crops.shape[0],1,1,1)).cuda()\n",
        "\n",
        "  for ns in range(num_rands):\n",
        "    randstotal*=torch.rand((img_crops.shape[0],1,1,1)).cuda()\n",
        "\n",
        "  img_crops = img_crops + noise_factor*randstotal*randnormal\n",
        "\n",
        "  return img_crops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWgi52zZV7oO"
      },
      "outputs": [],
      "source": [
        "def showme(Params, show_crop):\n",
        "  with torch.no_grad():\n",
        "    generated = generator(Params())\n",
        "\n",
        "    if (show_crop):\n",
        "      print(\"Augmented cropped example\")\n",
        "      aug_gen = generated.float()\n",
        "      aug_gen = create_crops(aug_gen, num_crops=1)\n",
        "      aug_gen_norm = norm_data(aug_gen[0])\n",
        "      show_from_tensor(aug_gen_norm)\n",
        "\n",
        "    print(\"Generation\")\n",
        "    latest_gen=norm_data(generated.cpu())\n",
        "    show_from_tensor(latest_gen[0])\n",
        "\n",
        "  return (latest_gen[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OZWVXThTZIk"
      },
      "outputs": [],
      "source": [
        "def optimize_result(Params, prompt):\n",
        "  alpha=1\n",
        "  beta=.5\n",
        "\n",
        "\n",
        "  out = generator(Params())\n",
        "  out = norm_data(out)\n",
        "  out = create_crops(out)\n",
        "  out = normalize(out)\n",
        "  image_enc=clipmodel.encode_image(out)\n",
        "\n",
        "\n",
        "  final_enc = w1*prompt + w2*extras_enc\n",
        "  final_text_include_enc = final_enc / final_enc.norm(dim=-1, keepdim=True)\n",
        "  final_text_exclude_enc = exclude_enc\n",
        "\n",
        "\n",
        "  main_loss = torch.cosine_similarity(final_text_include_enc, image_enc, -1)\n",
        "  penalize_loss = torch.cosine_similarity(final_text_exclude_enc, image_enc, -1)\n",
        "\n",
        "  final_loss = -alpha*main_loss + beta*penalize_loss\n",
        "\n",
        "  return final_loss\n",
        "\n",
        "def optimize(Params, optimizer, prompt):\n",
        "  loss = optimize_result(Params, prompt).mean()\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN3nPsZLTavu"
      },
      "outputs": [],
      "source": [
        "def training_loop(show_step, show_crop=False):\n",
        "  res_img=[]\n",
        "  res_z=[]\n",
        "\n",
        "  for prompt in include_enc:\n",
        "    iteration=0\n",
        "    Params, optimizer = init_params()\n",
        "\n",
        "    for it in range(total_iter):\n",
        "      loss = optimize(Params, optimizer, prompt)\n",
        "\n",
        "      if iteration>=80 and iteration%show_step == 0:\n",
        "        new_img = showme(Params, show_crop)\n",
        "        res_img.append(new_img)\n",
        "        res_z.append(Params())\n",
        "        print(\"loss:\", loss.item(), \"\\niteration:\",iteration)\n",
        "\n",
        "      iteration+=1\n",
        "    torch.cuda.empty_cache()\n",
        "  return res_img, res_z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "94fdILI2WBUY"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "include=['A dog on the beach Unreal Engine']\n",
        "exclude='Sharp image texture'\n",
        "extras = \"\"\n",
        "\n",
        "include_enc, exclude_enc, extras_enc = createEncodings(include, exclude, extras)\n",
        "res_img, res_z=training_loop(show_step, show_crop=False)\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}